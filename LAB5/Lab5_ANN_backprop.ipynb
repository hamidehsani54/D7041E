{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "#Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def f_relu(X, deriv=False):\n",
    "    if not deriv:\n",
    "        #return 0\n",
    "        return x*(x>0)\n",
    "    else:\n",
    "        ##return 1\n",
    "        return 1*(x>=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def exit_with_err(err_str):\n",
    "    print( sys.stderr, err_str)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        #calc the los function \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        #out put of back propaget, \n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            #calc the error of every node in layer\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            #delta *output, \n",
    "            #z is output\n",
    "            #T is transpos\n",
    "            #D is the diffrent what we get and what we sould get \n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.60727 Test error: 0.60420\n",
      "[   1]  Training error: 0.09308 Test error: 0.09270\n",
      "[   2]  Training error: 0.05703 Test error: 0.05920\n",
      "[   3]  Training error: 0.04360 Test error: 0.04790\n",
      "[   4]  Training error: 0.04623 Test error: 0.05100\n",
      "[   5]  Training error: 0.03793 Test error: 0.04390\n",
      "[   6]  Training error: 0.03648 Test error: 0.04550\n",
      "[   7]  Training error: 0.03270 Test error: 0.04160\n",
      "[   8]  Training error: 0.03137 Test error: 0.04060\n",
      "[   9]  Training error: 0.02605 Test error: 0.03830\n",
      "[  10]  Training error: 0.02390 Test error: 0.03830\n",
      "[  11]  Training error: 0.01705 Test error: 0.03310\n",
      "[  12]  Training error: 0.02028 Test error: 0.03590\n",
      "[  13]  Training error: 0.02223 Test error: 0.03850\n",
      "[  14]  Training error: 0.01588 Test error: 0.03340\n",
      "[  15]  Training error: 0.01520 Test error: 0.03260\n",
      "[  16]  Training error: 0.01910 Test error: 0.03680\n",
      "[  17]  Training error: 0.01398 Test error: 0.03330\n",
      "[  18]  Training error: 0.01173 Test error: 0.03290\n",
      "[  19]  Training error: 0.01027 Test error: 0.03370\n",
      "[  20]  Training error: 0.00930 Test error: 0.02990\n",
      "[  21]  Training error: 0.01543 Test error: 0.03730\n",
      "[  22]  Training error: 0.00947 Test error: 0.03190\n",
      "[  23]  Training error: 0.01175 Test error: 0.03310\n",
      "[  24]  Training error: 0.00918 Test error: 0.03200\n",
      "[  25]  Training error: 0.01330 Test error: 0.03530\n",
      "[  26]  Training error: 0.00957 Test error: 0.03100\n",
      "[  27]  Training error: 0.00793 Test error: 0.03300\n",
      "[  28]  Training error: 0.00775 Test error: 0.02970\n",
      "[  29]  Training error: 0.00637 Test error: 0.03090\n",
      "[  30]  Training error: 0.00447 Test error: 0.03090\n",
      "[  31]  Training error: 0.00753 Test error: 0.03360\n",
      "[  32]  Training error: 0.00380 Test error: 0.02900\n",
      "[  33]  Training error: 0.00555 Test error: 0.03160\n",
      "[  34]  Training error: 0.00548 Test error: 0.02990\n",
      "[  35]  Training error: 0.00740 Test error: 0.03140\n",
      "[  36]  Training error: 0.00865 Test error: 0.03330\n",
      "[  37]  Training error: 0.00743 Test error: 0.03030\n",
      "[  38]  Training error: 0.00737 Test error: 0.02980\n",
      "[  39]  Training error: 0.00805 Test error: 0.02970\n",
      "[  40]  Training error: 0.00518 Test error: 0.03080\n",
      "[  41]  Training error: 0.00680 Test error: 0.03040\n",
      "[  42]  Training error: 0.00475 Test error: 0.02900\n",
      "[  43]  Training error: 0.00318 Test error: 0.02870\n",
      "[  44]  Training error: 0.00127 Test error: 0.02830\n",
      "[  45]  Training error: 0.00130 Test error: 0.02860\n",
      "[  46]  Training error: 0.00085 Test error: 0.02930\n",
      "[  47]  Training error: 0.00100 Test error: 0.02830\n",
      "[  48]  Training error: 0.00082 Test error: 0.02800\n",
      "[  49]  Training error: 0.00017 Test error: 0.02770\n",
      "[  50]  Training error: 0.00008 Test error: 0.02730\n",
      "[  51]  Training error: 0.00008 Test error: 0.02690\n",
      "[  52]  Training error: 0.00008 Test error: 0.02660\n",
      "[  53]  Training error: 0.00005 Test error: 0.02660\n",
      "[  54]  Training error: 0.00005 Test error: 0.02690\n",
      "[  55]  Training error: 0.00005 Test error: 0.02680\n",
      "[  56]  Training error: 0.00005 Test error: 0.02670\n",
      "[  57]  Training error: 0.00003 Test error: 0.02690\n",
      "[  58]  Training error: 0.00002 Test error: 0.02710\n",
      "[  59]  Training error: 0.00000 Test error: 0.02710\n",
      "[  60]  Training error: 0.00000 Test error: 0.02720\n",
      "[  61]  Training error: 0.00000 Test error: 0.02720\n",
      "[  62]  Training error: 0.00000 Test error: 0.02720\n",
      "[  63]  Training error: 0.00000 Test error: 0.02720\n",
      "[  64]  Training error: 0.00000 Test error: 0.02730\n",
      "[  65]  Training error: 0.00000 Test error: 0.02750\n",
      "[  66]  Training error: 0.00000 Test error: 0.02760\n",
      "[  67]  Training error: 0.00000 Test error: 0.02760\n",
      "[  68]  Training error: 0.00000 Test error: 0.02770\n",
      "[  69]  Training error: 0.00000 Test error: 0.02770\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate =  0.005\n",
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.38868 Test error: 0.38990\n",
      "[   1]  Training error: 0.09987 Test error: 0.10030\n",
      "[   2]  Training error: 0.04682 Test error: 0.05060\n",
      "[   3]  Training error: 0.03870 Test error: 0.04470\n",
      "[   4]  Training error: 0.03220 Test error: 0.04110\n",
      "[   5]  Training error: 0.02465 Test error: 0.03460\n",
      "[   6]  Training error: 0.02543 Test error: 0.03680\n",
      "[   7]  Training error: 0.02243 Test error: 0.03430\n",
      "[   8]  Training error: 0.01920 Test error: 0.03330\n",
      "[   9]  Training error: 0.01650 Test error: 0.03060\n",
      "[  10]  Training error: 0.01640 Test error: 0.03200\n",
      "[  11]  Training error: 0.01473 Test error: 0.03100\n",
      "[  12]  Training error: 0.01540 Test error: 0.03240\n",
      "[  13]  Training error: 0.01460 Test error: 0.03320\n",
      "[  14]  Training error: 0.01450 Test error: 0.03220\n",
      "[  15]  Training error: 0.01885 Test error: 0.03650\n",
      "[  16]  Training error: 0.01133 Test error: 0.03040\n",
      "[  17]  Training error: 0.01258 Test error: 0.03040\n",
      "[  18]  Training error: 0.00843 Test error: 0.02910\n",
      "[  19]  Training error: 0.00962 Test error: 0.03170\n",
      "[  20]  Training error: 0.00632 Test error: 0.02960\n",
      "[  21]  Training error: 0.01012 Test error: 0.03240\n",
      "[  22]  Training error: 0.00735 Test error: 0.02840\n",
      "[  23]  Training error: 0.00645 Test error: 0.03020\n",
      "[  24]  Training error: 0.00628 Test error: 0.03170\n",
      "[  25]  Training error: 0.00968 Test error: 0.03360\n",
      "[  26]  Training error: 0.00375 Test error: 0.02730\n",
      "[  27]  Training error: 0.00415 Test error: 0.02880\n",
      "[  28]  Training error: 0.00505 Test error: 0.02880\n",
      "[  29]  Training error: 0.00253 Test error: 0.02940\n",
      "[  30]  Training error: 0.00320 Test error: 0.02790\n",
      "[  31]  Training error: 0.00272 Test error: 0.02840\n",
      "[  32]  Training error: 0.01407 Test error: 0.03330\n",
      "[  33]  Training error: 0.00813 Test error: 0.02940\n",
      "[  34]  Training error: 0.00465 Test error: 0.02750\n",
      "[  35]  Training error: 0.00572 Test error: 0.02880\n",
      "[  36]  Training error: 0.00418 Test error: 0.02900\n",
      "[  37]  Training error: 0.00418 Test error: 0.02810\n",
      "[  38]  Training error: 0.00225 Test error: 0.02660\n",
      "[  39]  Training error: 0.00248 Test error: 0.02560\n",
      "[  40]  Training error: 0.00160 Test error: 0.02710\n",
      "[  41]  Training error: 0.00093 Test error: 0.02570\n",
      "[  42]  Training error: 0.00130 Test error: 0.02570\n",
      "[  43]  Training error: 0.00107 Test error: 0.02630\n",
      "[  44]  Training error: 0.00040 Test error: 0.02590\n",
      "[  45]  Training error: 0.00018 Test error: 0.02580\n",
      "[  46]  Training error: 0.00005 Test error: 0.02560\n",
      "[  47]  Training error: 0.00002 Test error: 0.02530\n",
      "[  48]  Training error: 0.00002 Test error: 0.02530\n",
      "[  49]  Training error: 0.00002 Test error: 0.02530\n",
      "[  50]  Training error: 0.00002 Test error: 0.02530\n",
      "[  51]  Training error: 0.00002 Test error: 0.02500\n",
      "[  52]  Training error: 0.00002 Test error: 0.02490\n",
      "[  53]  Training error: 0.00000 Test error: 0.02530\n",
      "[  54]  Training error: 0.00000 Test error: 0.02530\n",
      "[  55]  Training error: 0.00000 Test error: 0.02530\n",
      "[  56]  Training error: 0.00000 Test error: 0.02500\n",
      "[  57]  Training error: 0.00000 Test error: 0.02490\n",
      "[  58]  Training error: 0.00000 Test error: 0.02500\n",
      "[  59]  Training error: 0.00000 Test error: 0.02510\n",
      "[  60]  Training error: 0.00000 Test error: 0.02510\n",
      "[  61]  Training error: 0.00000 Test error: 0.02510\n",
      "[  62]  Training error: 0.00000 Test error: 0.02520\n",
      "[  63]  Training error: 0.00000 Test error: 0.02520\n",
      "[  64]  Training error: 0.00000 Test error: 0.02520\n",
      "[  65]  Training error: 0.00000 Test error: 0.02510\n",
      "[  66]  Training error: 0.00000 Test error: 0.02510\n",
      "[  67]  Training error: 0.00000 Test error: 0.02510\n",
      "[  68]  Training error: 0.00000 Test error: 0.02490\n",
      "[  69]  Training error: 0.00000 Test error: 0.02490\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eta = 0.005\n",
    "print(\"Learning rate = \", eta)\n",
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
